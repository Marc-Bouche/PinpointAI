{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbdf060-918f-49e5-a82c-2079c1ef3f5e",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">**PinpointAI | Part II**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6ea06-a4db-4510-afc0-4eda4dc5f7a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CSV Import form Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "53dbe927-4019-4204-a3e7-67f0957f144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1df0a19-02c8-486b-80a7-c4f7dc970654",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr1.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_1 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_1.head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "977c9bce-9f26-4dc2-ae59-72f00c855d09",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr2.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_2 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_2.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ad894fc-5c74-4d14-87c8-933864468d0b",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr3.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_3 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_3.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fc5f584-0968-41a9-ab10-212ef73882d8",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr4.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_4 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_4.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eeeadb64-75ca-4bea-96e3-ff7bf2368e53",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr5.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_5 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_5.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85f5e673-306d-4f2a-ae11-a8df0859825c",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr6.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_6 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_6.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8753b93e-d760-437d-9f04-d44c05993ede",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/valid_coordinates_fr7.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df_coordinates_7 = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "print(df_coordinates_7.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c93b3-fbdd-4987-b7d3-80be576757d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat Data frames"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1de53f04-e92e-4a3a-b9a3-cf82494d8850",
   "metadata": {},
   "source": [
    "# Ensure all DataFrames are available: df_coordinates_1, df_coordinates_2, df_coordinates_3, df_coordinates_4, df_coordinates_5, df_coordinates_6,df_coordinates_7\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df_combined = pd.concat([df_coordinates_1, df_coordinates_2, df_coordinates_3, df_coordinates_4, df_coordinates_5, df_coordinates_6, df_coordinates_7], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3857bcc5-355b-4412-88d0-9bbcae5ef5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the combined DataFrame to a new CSV\n",
    "df_combined.to_csv('/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/combined_coordinates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c9129f32-0037-4a23-94ed-55b66d0b3e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186e261-7976-40a5-8777-250bd4220a4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Streetview Images Collection"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f9b7077-1a91-4d2e-aca1-d8cacb33d583",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import requests\n",
    "from streetview_api import API_KEY\n",
    "\n",
    "\n",
    "# Path to the folder where images will be stored\n",
    "output_folder = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/jpeg_streetview_image'\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Slice the DataFrame to start from row 10011\n",
    "df_coordinates = df_coordinates.iloc[10010:]  # Pandas uses zero-based indexing\n",
    "\n",
    "# Load the coordinates from the CSV file\n",
    "combined_coordinates_path = \"combined_coordinates.csv\"\n",
    "df_coordinates = pd.read_csv(combined_coordinates_path)\n",
    "\n",
    "# Extract coordinates\n",
    "coordinates = list(zip(df_coordinates['Latitude'], df_coordinates['Longitude']))\n",
    "\n",
    "# Function to download Street View image\n",
    "def download_streetview_image(lat, lon, filename):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/streetview?size=600x400&location={lat},{lon}&key={API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Start the counter at 11162\n",
    "counter = 11162\n",
    "\n",
    "# Collect images and store information\n",
    "for lat, lon in coordinates:\n",
    "    filename = os.path.join(output_folder, f\"streetview_{counter}.jpeg\")  # Change .jpg to .jpeg\n",
    "    success = download_streetview_image(lat, lon, filename)\n",
    "    if success:\n",
    "        data.append({\n",
    "            \"image_filename\": filename,\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon\n",
    "        })\n",
    "        print(f\"Downloaded image {filename} for coordinates ({lat}, {lon})\")\n",
    "        counter += 1  # Increment the counter for the next image\n",
    "    else:\n",
    "        print(f\"Failed to download image for coordinates ({lat}, {lon})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720d875-777c-4ebc-9867-1266f74f4221",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data set duplicate cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2364c4b3-8abf-434e-8563-f88c1e45d3b7",
   "metadata": {},
   "source": [
    "# Define the path to your CSV file\n",
    "csv_file_path = '/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/combined_coordinates.csv'  # Replace with your actual file path\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "combined_coordinates = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it loaded correctly\n",
    "combined_coordinates.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d8b55dd-1e00-4fce-af08-5d9834419213",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Check for duplicates in the entire dataframe\n",
    "duplicates = combined_coordinates.duplicated()\n",
    "\n",
    "# Display duplicates\n",
    "print(duplicates)\n",
    "\n",
    "# Count the number of duplicates\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicates in the first 5 rows: {num_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd594683-8227-4aab-bfc7-8354e77e01f5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image Integrity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fc63fb4b-1ebe-4b94-825f-3d1d1aad50fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-image file: .DS_Store\n",
      "Skipping non-image file: .ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = \"/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/jpeg_streetview_image\"\n",
    "\n",
    "# Supported image extensions\n",
    "valid_extensions = (\".jpg\", \".jpeg\", \".png\")\n",
    "\n",
    "# Check for corrupted images\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.lower().endswith(valid_extensions):  # Check if the file is an image\n",
    "        try:\n",
    "            img = Image.open(os.path.join(dataset_path, filename))\n",
    "            img.verify()  # Verify image integrity\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f\"Corrupted image: {filename}\")\n",
    "    else:\n",
    "        print(f\"Skipping non-image file: {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b5c56c-ede6-4e6e-9d84-8eb3314d485f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image Preprocessing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f46927d4-9672-46b1-85dd-beed928b1cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278eea6-34ef-4c9e-87aa-b2e4a75e9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "dataset_path = \"/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/jpeg_streetview_image\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Preprocess and save all images as tensors\n",
    "preprocessed_images = []  # To store processed tensors\n",
    "image_labels = []  # Assuming you have corresponding labels for each image\n",
    "\n",
    "# Supported image extensions\n",
    "valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\", \".gif\")\n",
    "\n",
    "for filename in os.listdir(dataset_path):\n",
    "    if filename.lower().endswith(valid_extensions):  # Check if the file is an image\n",
    "        try:\n",
    "            image_path = os.path.join(dataset_path, filename)\n",
    "            image = Image.open(image_path).convert(\"RGB\")  # Ensure image has 3 color channels\n",
    "            processed_image = transform(image)\n",
    "            preprocessed_images.append(processed_image)\n",
    "            \n",
    "            # Append corresponding label if available (optional, depends on your dataset structure)\n",
    "            # Example: Extract label from filename or a separate label file\n",
    "            # label = extract_label_from_filename(filename)  # Placeholder function\n",
    "            # image_labels.append(label)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Convert the list to a tensor batch for easier handling (optional)\n",
    "if preprocessed_images:\n",
    "    batched_images = torch.stack(preprocessed_images)\n",
    "    print(f\"Processed {len(preprocessed_images)} images successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a7cc7-6232-41cc-b6dc-8f4b3f4a547f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Visualize Processed Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da37cbb6-85ad-49d6-8660-67ba993c9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualize image tensors or labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display a few processed images (unnormalized for visualization)\n",
    "def show_image(tensor):\n",
    "    tensor = tensor.permute(1, 2, 0)  # Change dimension order to HxWxC\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])  # De-normalize\n",
    "    tensor = torch.clip(tensor, 0, 1)  # Ensure pixel values are between 0 and 1\n",
    "    plt.imshow(tensor)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_image(preprocessed_images[0])  # Display first image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754dbc65-638c-4f3b-8200-dc09688bf630",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Split Data\n",
    "- Divide your dataset into training, validation, and test sets (e.g., 70% training, 20% validation, 10% test).\n",
    "- Ensure the split is random but balanced across regions or coordinate ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248d3eb-4482-4918-b277-597b9df96cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example split (modify if you have labels)\n",
    "train_images, val_images = train_test_split(preprocessed_images, test_size=0.2, random_state=42)\n",
    "val_images, test_images = train_test_split(val_images, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_images)}\")\n",
    "print(f\"Validation set size: {len(val_images)}\")\n",
    "print(f\"Test set size: {len(test_images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc39237-d8fc-44d3-8a6e-036291dc27d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe876f-d04f-40e5-91cd-a662ddc0ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Ensure image_paths[idx] is a valid path\n",
    "        image_path = self.image_paths[idx]\n",
    "\n",
    "        # Open the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure it's in RGB format\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222c3da-a699-4c51-9cef-f21870ae6b4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Augmentation and Transformation for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3798c6-6ba4-4263-bf6d-e15b2ddbb07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),  # Augmentation for training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafd05a-5a2c-4a43-8369-a349dc0125da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading Dataset using ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7e00d-e70d-4873-ace6-0ed214d59b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define your dataset (replace with your own data path)\n",
    "train_dataset = datasets.ImageFolder(root='/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/jpeg_streetview_image', transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root='/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/jpeg_streetview_image', transform=val_test_transform)\n",
    "test_dataset = datasets.ImageFolder(root='/Users/mbouch17/Desktop/Personal_Data_Project/PinpointAI/jpeg_streetview_image', transform=val_test_transform)\n",
    "\n",
    "# Define DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f706b6f-21a7-4c85-b27a-85cff42917e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DataLoader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1b646-e303-4a05-bc0e-9a5ea026f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64  # Adjust based on your GPU/CPU memory\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88bdef-ed5a-4b65-98ec-cb3cd3fa8dc3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Definition with Pre-trained ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7f167-f382-4995-aef3-c35e7889dcba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load pre-trained ResNet18 with updated weights parameter\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1  # Equivalent to the old 'pretrained=True'\n",
    "model = resnet18(weights=weights)\n",
    "\n",
    "# Replace the final fully connected layer\n",
    "num_classes = 100  # Example: Adjust based on your dataset\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e420e-dcfc-4d5a-8a9f-38b854614e2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2510bf34-c9fb-4261-bc58-0fb725873eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()  # For classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(10):  # Adjust the number of epochs\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch  # Unpack images and labels\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)  # Move labels to the same device as the model\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)  # Use the actual labels\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {train_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c5fdb-2edc-4782-8657-550dbf1f282a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Freezing Pre-trained Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5cc603-dd4c-4c52-83ec-6bbbe411c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze earlier layers\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True  # Unfreeze the classifier layer\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03131f7d-0f94-4eec-b5de-8954bc89197b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training Loop with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b2cc9-b493-4ab2-8bc7-5c85e532d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Adjust based on your needs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78201c3b-e540-4e86-af8b-1b3ae3902f77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c883345-6226-4de1-8db9-1272a8f7e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss / len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047af7f-6fe1-4856-bd1e-80bbb0c07465",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc98a0a-dbf2-4001-b75e-d06da2e61168",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6838467-1df7-4805-bc03-fd7907178fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to denormalize the image\n",
    "def denormalize_image(tensor, mean, std):\n",
    "    mean = torch.tensor(mean).view(1, 3, 1, 1)\n",
    "    std = torch.tensor(std).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Reverse the normalization\n",
    "    return tensor\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Denormalize the image for visualization\n",
    "        denormalized_image = denormalize_image(images[0], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        denormalized_image = denormalized_image.squeeze().cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
    "\n",
    "        # Clip values to be in range [0, 1]\n",
    "        denormalized_image = np.clip(denormalized_image, 0, 1)\n",
    "\n",
    "        # Display the image\n",
    "        plt.imshow(denormalized_image)\n",
    "        plt.title(f\"True: {labels[0].item()}, Predicted: {predicted[0].item()}\")\n",
    "        plt.show()\n",
    "        break  # Display just one image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debb641-9985-4e86-94f6-f730bc643ee7",
   "metadata": {},
   "source": [
    "## Save & Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37e7cf-ff09-4116-aeee-3245ff29b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model.state_dict(), \"fine_tuned_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa77c12-ece4-4393-96b0-81ddac1f556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"fine_tuned_model.pth\", weights_only=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
